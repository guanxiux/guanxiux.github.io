@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = CVPR,
pages = {234--778},
year = 2005
}


@inproceedings{sucar_imap_2021,
	address = {Montreal, QC, Canada},
	title = {{iMAP}: {Implicit} {Mapping} and {Positioning} in {Real}-{Time}},
	isbn = {978-1-66542-812-5},
	shorttitle = {{iMAP}},
	url = {https://ieeexplore.ieee.org/document/9710431/},
	doi = {10.1109/ICCV48922.2021.00617},
	abstract = {We show for the ﬁrst time that a multilayer perceptron (MLP) can serve as the only scene representation in a realtime SLAM system for a handheld RGB-D camera. Our network is trained in live operation without prior data, building a dense, scene-speciﬁc implicit 3D model of occupancy and colour which is also immediately used for tracking.},
	language = {en},
	urldate = {2022-06-30},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Sucar, Edgar and Liu, Shikun and Ortiz, Joseph and Davison, Andrew J.},
	month = oct,
	year = {2021},
	pages = {6209--6218},
	file = {Sucar et al_2021_iMAP.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\SLAM\\explicit slam\\Sucar et al_2021_iMAP2.pdf:application/pdf},
}

@inproceedings{zhu_nice-slam_2022,
	title = {{NICE}-{SLAM}: {Neural} {Implicit} {Scalable} {Encoding} for {SLAM}},
	shorttitle = {{NICE}-{SLAM}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Zhu_NICE-SLAM_Neural_Implicit_Scalable_Encoding_for_SLAM_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-09-09},
	author = {Zhu, Zihan and Peng, Songyou and Larsson, Viktor and Xu, Weiwei and Bao, Hujun and Cui, Zhaopeng and Oswald, Martin R. and Pollefeys, Marc},
	year = {2022},
	pages = {12786--12796},
	file = {Snapshot:C\:\\Users\\guanxiuxian\\Zotero\\storage\\LPEWTUSF\\Zhu_NICE-SLAM_Neural_Implicit_Scalable_Encoding_for_SLAM_CVPR_2022_paper.html:text/html;Zhu et al_2022_NICE-SLAM.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\SLAM\\explicit slam\\Zhu et al_2022_NICE-SLAM2.pdf:application/pdf},
}

@inproceedings{li_bnv-fusion_2022,
	address = {New Orleans, LA, USA},
	title = {{BNV}-{Fusion}: {Dense} {3D} {Reconstruction} using {Bi}-level {Neural} {Volume} {Fusion}},
	isbn = {978-1-66546-946-3},
	shorttitle = {{BNV}-{Fusion}},
	url = {https://ieeexplore.ieee.org/document/9880298/},
	doi = {10.1109/CVPR52688.2022.00607},
	abstract = {Dense 3D reconstruction from a stream of depth images is the key to many mixed reality and robotic applications. Although methods based on Truncated Signed Distance Function (TSDF) Fusion have advanced the ﬁeld over the years, the TSDF volume representation is confronted with striking a balance between the robustness to noisy measurements and maintaining the level of detail. We present Bi-level Neural Volume Fusion (BNV-Fusion), which leverages recent advances in neural implicit representations and neural rendering for dense 3D reconstruction. In order to incrementally integrate new depth maps into a global neural implicit representation, we propose a novel bi-level fusion strategy that considers both efﬁciency and reconstruction quality by design. We evaluate the proposed method on multiple datasets quantitatively and qualitatively, demonstrating a signiﬁcant improvement over existing methods.},
	language = {en},
	urldate = {2023-05-24},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Li, Kejie and Tang, Yansong and Prisacariu, Victor Adrian and Torr, Philip H.S.},
	month = jun,
	year = {2022},
	pages = {6156--6165},
	file = {Li 等 - 2022 - BNV-Fusion Dense 3D Reconstruction using Bi-level.pdf:C\:\\Users\\guanxiuxian\\Zotero\\storage\\TLE3YM3J\\Li 等 - 2022 - BNV-Fusion Dense 3D Reconstruction using Bi-level.pdf:application/pdf},
}

@incollection{avidan_activenerf_2022,
	address = {Cham},
	title = {{ActiveNeRF}: {Learning} {Where} to {See} with {Uncertainty} {Estimation}},
	volume = {13693},
	isbn = {978-3-031-19826-7 978-3-031-19827-4},
	shorttitle = {{ActiveNeRF}},
	url = {https://link.springer.com/10.1007/978-3-031-19827-4_14},
	abstract = {Recently, Neural Radiance Fields (NeRF) has shown promising performances on reconstructing 3D scenes and synthesizing novel views from a sparse set of 2D images. Albeit effective, the performance of NeRF is highly influenced by the quality of training samples. With limited posed images from the scene, NeRF fails to generalize well to novel views and may collapse to trivial solutions in unobserved regions. This makes NeRF impractical under resource-constrained scenarios. In this paper, we present a novel learning framework, ActiveNeRF, aiming to model a 3D scene with a constrained input budget. Specifically, we first incorporate uncertainty estimation into a NeRF model, which ensures robustness under few observations and provides an interpretation of how NeRF understands the scene. On this basis, we propose to supplement the existing training set with newly captured samples based on an active learning scheme. By evaluating the reduction of uncertainty given new inputs, we select the samples that bring the most information gain. In this way, the quality of novel view synthesis can be improved with minimal additional resources. Extensive experiments validate the performance of our model on both realistic and synthetic scenes, especially with scarcer training data.},
	language = {en},
	urldate = {2023-08-04},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Pan, Xuran and Lai, Zihang and Song, Shiji and Huang, Gao},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	doi = {10.1007/978-3-031-19827-4_14},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {230--246},
	file = {Pan 等 - 2022 - ActiveNeRF Learning Where to See with Uncertainty.pdf:C\:\\Users\\guanxiuxian\\Zotero\\storage\\IL2A6DYY\\Pan 等 - 2022 - ActiveNeRF Learning Where to See with Uncertainty.pdf:application/pdf},
}

@misc{mildenhall_nerf_2020,
	title = {{NeRF}: {Representing} {Scenes} as {Neural} {Radiance} {Fields} for {View} {Synthesis}},
	shorttitle = {{NeRF}},
	url = {http://arxiv.org/abs/2003.08934},
	doi = {10.48550/arXiv.2003.08934},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$({\textbackslash}theta, {\textbackslash}phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
	urldate = {2023-08-06},
	publisher = {arXiv},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	month = aug,
	year = {2020},
	note = {arXiv:2003.08934 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	file = {arXiv.org Snapshot:C\:\\Users\\guanxiuxian\\Zotero\\storage\\VA7U3NYF\\2003.html:text/html;Mildenhall et al_2020_NeRF.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\AAAI\\reconstruction\\Mildenhall et al_2020_NeRF.pdf:application/pdf},
}

@misc{tran_enhanced_2023,
	title = {An {Enhanced} {Sampling}-{Based} {Method} {With} {Modified} {Next}-{Best} {View} {Strategy} {For} {2D} {Autonomous} {Robot} {Exploration}},
	url = {http://arxiv.org/abs/2305.04576},
	doi = {10.48550/arXiv.2305.04576},
	abstract = {Autonomous exploration is a new technology in the field of robotics that has found widespread application due to its objective to help robots independently localize, scan maps, and navigate any terrain without human control. Up to present, the sampling-based exploration strategies have been the most effective for aerial and ground vehicles equipped with depth sensors producing three-dimensional point clouds. Those methods utilize the sampling task to choose random points or make samples based on Rapidly-exploring Random Trees (RRT). Then, they decide on frontiers or Next Best Views (NBV) with useful volumetric information. However, most state-of-the-art sampling-based methodology is challenging to implement in two-dimensional robots due to the lack of environmental knowledge, thus resulting in a bad volumetric gain for evaluating random destinations. This study proposed an enhanced sampling-based solution for indoor robot exploration to decide Next Best View (NBV) in 2D environments. Our method makes RRT until have the endpoints as frontiers and evaluates those with the enhanced utility function. The volumetric information obtained from environments was estimated using non-uniform distribution to determine cells that are occupied and have an uncertain probability. Compared to the sampling-based Frontier Detection and Receding Horizon NBV approaches, the methodology executed performed better in Gazebo platform-simulated environments, achieving a significantly larger explored area, with the average distance and time traveled being reduced. Moreover, the operated proposed method on an author-built 2D robot exploring the entire natural environment confirms that the method is effective and applicable in real-world scenarios.},
	urldate = {2023-08-06},
	publisher = {arXiv},
	author = {Tran, Dong Huu Quoc and Phan, Hoang-Anh and Van, Hieu Dang and Van Duong, Tan and Bui, Tung Thanh and Thanh, Van Nguyen Thi},
	month = may,
	year = {2023},
	note = {arXiv:2305.04576 [cs]},
	keywords = {Computer Science - Robotics},
	file = {arXiv.org Snapshot:C\:\\Users\\guanxiuxian\\Zotero\\storage\\RLJVRRQ2\\2305.html:text/html;Tran et al_2023_An Enhanced Sampling-Based Method With Modified Next-Best View Strategy For 2D.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\AAAI\\NBV\\Tran et al_2023_An Enhanced Sampling-Based Method With Modified Next-Best View Strategy For 2D.pdf:application/pdf},
}


@article{guedon_scone_2022,
	title = {{SCONE}: {Surface} {Coverage} {Optimization} in {Unknown} {Environments} by {Volumetric} {Integration}},
	volume = {35},
	shorttitle = {{SCONE}},
	url = {https://proceedings.neurips.cc//paper_files/paper/2022/hash/828c6d69bdf91fca7f2b97c4dc214e94-Abstract-Conference.html},
	language = {en},
	urldate = {2023-06-09},
	journal = {Advances in Neural Information Processing Systems},
	author = {Guedon, Antoine and Monasse, Pascal and Lepetit, Vincent},
	month = dec,
	year = {2022},
	pages = {20731--20743},
	file = {Guedon et al_2022_SCONE.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\maol\\baselines\\Guedon et al_2022_SCONE.pdf:application/pdf},
}

@misc{ran_neurar_2023,
	title = {{NeurAR}: {Neural} {Uncertainty} for {Autonomous} {3D} {Reconstruction} with {Implicit} {Neural} {Representations}},
	shorttitle = {{NeurAR}},
	url = {http://arxiv.org/abs/2207.10985},
	doi = {10.1109/LRA.2023.3235686},
	abstract = {Implicit neural representations have shown compelling results in offline 3D reconstruction and also recently demonstrated the potential for online SLAM systems. However, applying them to autonomous 3D reconstruction, where a robot is required to explore a scene and plan a view path for the reconstruction, has not been studied. In this paper, we explore for the first time the possibility of using implicit neural representations for autonomous 3D scene reconstruction by addressing two key challenges: 1) seeking a criterion to measure the quality of the candidate viewpoints for the view planning based on the new representations, and 2) learning the criterion from data that can generalize to different scenes instead of a hand-crafting one. To solve the challenges, firstly, a proxy of Peak Signal-to-Noise Ratio (PSNR) is proposed to quantify a viewpoint quality; secondly, the proxy is optimized jointly with the parameters of an implicit neural network for the scene. With the proposed view quality criterion from neural networks (termed as Neural Uncertainty), we can then apply implicit representations to autonomous 3D reconstruction. Our method demonstrates significant improvements on various metrics for the rendered image quality and the geometry quality of the reconstructed 3D models when compared with variants using TSDF or reconstruction without view planning. Project webpage https://kingteeloki-ran.github.io/NeurAR/},
	urldate = {2023-06-30},
	author = {Ran, Yunlong and Zeng, Jing and He, Shibo and Li, Lincheng and Chen, Yingfeng and Lee, Gimhee and Chen, Jiming and Ye, Qi},
	month = feb,
	year = {2023},
	note = {arXiv:2207.10985 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\guanxiuxian\\Zotero\\storage\\DJ2E7B7B\\2207.html:text/html;Full Text PDF:C\:\\Users\\guanxiuxian\\Zotero\\storage\\BCU3PUUZ\\Ran 等 - 2023 - NeurAR Neural Uncertainty for Autonomous 3D Recon.pdf:application/pdf},
}

@misc{jin_neu-nbv_2023,
	title = {{NeU}-{NBV}: {Next} {Best} {View} {Planning} {Using} {Uncertainty} {Estimation} in {Image}-{Based} {Neural} {Rendering}},
	shorttitle = {{NeU}-{NBV}},
	url = {http://arxiv.org/abs/2303.01284},
	doi = {10.48550/arXiv.2303.01284},
	abstract = {Autonomous robotic tasks require actively perceiving the environment to achieve application-specific goals. In this paper, we address the problem of positioning an RGB camera to collect the most informative images to represent an unknown scene, given a limited measurement budget. We propose a novel mapless planning framework to iteratively plan the next best camera view based on collected image measurements. A key aspect of our approach is a new technique for uncertainty estimation in image-based neural rendering, which guides measurement acquisition at the most uncertain view among view candidates, thus maximising the information value during data collection. By incrementally adding new measurements into our image collection, our approach efficiently explores an unknown scene in a mapless manner. We show that our uncertainty estimation is generalisable and valuable for view planning in unknown scenes. Our planning experiments using synthetic and real-world data verify that our uncertainty-guided approach finds informative images leading to more accurate scene representations when compared against baselines.},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Jin, Liren and Chen, Xieyuanli and Rückin, Julius and Popović, Marija},
	month = jul,
	year = {2023},
	note = {arXiv:2303.01284 [cs]},
	keywords = {Computer Science - Robotics},
	file = {arXiv.org Snapshot:C\:\\Users\\guanxiuxian\\Zotero\\storage\\B3ACI6AP\\2303.html:text/html;Jin et al_2023_NeU-NBV.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\AAAI\\uncertainty\\Jin et al_2023_NeU-NBV.pdf:application/pdf},
}


@misc{shen_stochastic_2021,
	title = {Stochastic {Neural} {Radiance} {Fields}: {Quantifying} {Uncertainty} in {Implicit} {3D} {Representations}},
	shorttitle = {Stochastic {Neural} {Radiance} {Fields}},
	url = {http://arxiv.org/abs/2109.02123},
	doi = {10.48550/arXiv.2109.02123},
	abstract = {Neural Radiance Fields (NeRF) has become a popular framework for learning implicit 3D representations and addressing different tasks such as novel-view synthesis or depth-map estimation. However, in downstream applications where decisions need to be made based on automatic predictions, it is critical to leverage the confidence associated with the model estimations. Whereas uncertainty quantification is a long-standing problem in Machine Learning, it has been largely overlooked in the recent NeRF literature. In this context, we propose Stochastic Neural Radiance Fields (S-NeRF), a generalization of standard NeRF that learns a probability distribution over all the possible radiance fields modeling the scene. This distribution allows to quantify the uncertainty associated with the scene information provided by the model. S-NeRF optimization is posed as a Bayesian learning problem which is efficiently addressed using the Variational Inference framework. Exhaustive experiments over benchmark datasets demonstrate that S-NeRF is able to provide more reliable predictions and confidence values than generic approaches previously proposed for uncertainty estimation in other domains.},
	urldate = {2023-08-06},
	publisher = {arXiv},
	author = {Shen, Jianxiong and Ruiz, Adria and Agudo, Antonio and Moreno-Noguer, Francesc},
	month = sep,
	year = {2021},
	note = {arXiv:2109.02123 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\guanxiuxian\\Zotero\\storage\\8UUEKRBB\\2109.html:text/html;Shen et al_2021_Stochastic Neural Radiance Fields.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\AAAI\\NBV\\Shen et al_2021_Stochastic Neural Radiance Fields.pdf:application/pdf},
}

@article{badings_probabilities_2023,
	title = {Probabilities {Are} {Not} {Enough}: {Formal} {Controller} {Synthesis} for {Stochastic} {Dynamical} {Models} with {Epistemic} {Uncertainty}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Probabilities {Are} {Not} {Enough}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/26718},
	doi = {10.1609/aaai.v37i12.26718},
	abstract = {Capturing uncertainty in models of complex dynamical systems is crucial to designing safe controllers. Stochastic noise causes aleatoric uncertainty, whereas imprecise knowledge of model parameters leads to epistemic uncertainty. Several approaches use formal abstractions to synthesize policies that satisfy temporal specifications related to safety and reachability. However, the underlying models exclusively capture aleatoric but not epistemic uncertainty, and thus require that model parameters are known precisely. Our contribution to overcoming this restriction is a novel abstraction-based controller synthesis method for continuous-state models with stochastic noise and uncertain parameters. By sampling techniques and robust analysis, we capture both aleatoric and epistemic uncertainty, with a user-specified confidence level, in the transition probability intervals of a so-called interval Markov decision process (iMDP). We synthesize an optimal policy on this iMDP, which translates (with the specified confidence level) to a feedback controller for the continuous model with the same performance guarantees. Our experimental benchmarks confirm that accounting for epistemic uncertainty leads to controllers that are more robust against variations in parameter values.},
	language = {en},
	number = {12},
	urldate = {2023-08-01},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Badings, Thom and Romao, Licio and Abate, Alessandro and Jansen, Nils},
	month = jun,
	year = {2023},
	note = {Number: 12},
	keywords = {General},
	pages = {14701--14710},
	file = {Badings et al_2023_Probabilities Are Not Enough.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\AAAI\\uncertainty\\Badings et al_2023_Probabilities Are Not Enough.pdf:application/pdf},
}

@article{berry_normalizing_2023,
	title = {Normalizing {Flow} {Ensembles} for {Rich} {Aleatoric} and {Epistemic} {Uncertainty} {Modeling}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/25834},
	doi = {10.1609/aaai.v37i6.25834},
	abstract = {In this work, we demonstrate how to reliably estimate epistemic uncertainty while maintaining the flexibility needed to capture complicated aleatoric distributions. To this end, we propose an ensemble of Normalizing Flows (NF), which are state-of-the-art in modeling aleatoric uncertainty. The ensembles are created via sets of fixed dropout masks, making them less expensive than creating separate NF models. We demonstrate how to leverage the unique structure of NFs, base distributions, to estimate aleatoric uncertainty without relying on samples, provide a comprehensive set of baselines, and derive unbiased estimates for differential entropy. The methods were applied to a variety of experiments, commonly used to benchmark aleatoric and epistemic uncertainty estimation: 1D sinusoidal data, 2D windy grid-world (Wet Chicken), Pendulum, and Hopper. In these experiments, we setup an active learning framework and evaluate each model's capability at measuring aleatoric and epistemic uncertainty. The results show the advantages of using NF ensembles in capturing complicated aleatoric while maintaining accurate epistemic uncertainty estimates.},
	language = {en},
	number = {6},
	urldate = {2023-08-01},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Berry, Lucas and Meger, David},
	month = jun,
	year = {2023},
	note = {Number: 6},
	keywords = {ML: Probabilistic Methods},
	pages = {6806--6814},
	file = {Berry_Meger_2023_Normalizing Flow Ensembles for Rich Aleatoric and Epistemic Uncertainty Modeling.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\AAAI\\uncertainty\\Berry_Meger_2023_Normalizing Flow Ensembles for Rich Aleatoric and Epistemic Uncertainty Modeling.pdf:application/pdf},
}

@article{csillag_amnioml_2023,
	title = {{AmnioML}: {Amniotic} {Fluid} {Segmentation} and {Volume} {Prediction} with {Uncertainty} {Quantification}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {{AmnioML}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/26837},
	doi = {10.1609/aaai.v37i13.26837},
	abstract = {Accurately predicting the volume of amniotic fluid is fundamental to assessing pregnancy risks, though the task usually requires many hours of laborious work by medical experts. In this paper, we present  AmnioML, a machine learning solution that leverages deep learning and conformal prediction to output fast and accurate volume estimates and segmentation masks from fetal MRIs with Dice coefficient over 0.9. Also, we make available a novel, curated dataset for fetal MRIs with 853 exams and benchmark the performance of many recent deep learning architectures. In addition, we introduce a conformal prediction tool that yields narrow predictive intervals with theoretically guaranteed coverage, thus aiding doctors in detecting pregnancy risks and saving lives. A successful case study of AmnioML deployed in a medical setting is also reported. Real-world clinical benefits include up to 20x segmentation time reduction, with most segmentations deemed by doctors as not needing any further manual refinement. Furthermore, AmnioML's volume predictions were found to be highly accurate in practice, with mean absolute error below 56mL and tight predictive intervals, showcasing its impact in reducing pregnancy complications.},
	language = {en},
	number = {13},
	urldate = {2023-08-01},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Csillag, Daniel and Paes, Lucas Monteiro and Ramos, Thiago and Romano, João Vitor and Schuller, Rodrigo and Seixas, Roberto B. and Oliveira, Roberto I. and Orenstein, Paulo},
	month = jun,
	year = {2023},
	note = {Number: 13},
	keywords = {Medical Segmentation},
	pages = {15494--15502},
	file = {Csillag et al_2023_AmnioML.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\AAAI\\uncertainty\\Csillag et al_2023_AmnioML.pdf:application/pdf},
}

@article{fei_uncertainty-aware_2023,
	title = {Uncertainty-{Aware} {Image} {Captioning}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/25137},
	doi = {10.1609/aaai.v37i1.25137},
	abstract = {It is well believed that the higher uncertainty in a word of the caption, the more inter-correlated context information is required to determine it. However, current image captioning methods usually consider the generation of all words in a sentence sequentially and equally. In this paper, we propose an uncertainty-aware image captioning framework, which parallelly and iteratively operates insertion of discontinuous candidate words between existing words from easy to difficult until converged. We hypothesize that high-uncertainty words in a sentence need more prior information to make a correct decision and should be produced at a later stage. The resulting non-autoregressive hierarchy makes the caption generation explainable and intuitive. Specifically, we utilize an image-conditioned bag-of-word model to measure the word uncertainty and apply a dynamic programming algorithm to construct the training pairs. During inference, we devise an uncertainty-adaptive parallel beam search technique that yields an empirically logarithmic time complexity. Extensive experiments on the MS COCO benchmark reveal that our approach outperforms the strong baseline and related methods on both captioning quality as well as decoding speed.},
	language = {en},
	number = {1},
	urldate = {2023-08-01},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Fei, Zhengcong and Fan, Mingyuan and Zhu, Li and Huang, Junshi and Wei, Xiaoming and Wei, Xiaolin},
	month = jun,
	year = {2023},
	note = {Number: 1},
	keywords = {CV: Applications},
	pages = {614--622},
	file = {Fei et al_2023_Uncertainty-Aware Image Captioning.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\AAAI\\uncertainty\\Fei et al_2023_Uncertainty-Aware Image Captioning.pdf:application/pdf},
}

@article{grover_generative_2023,
	title = {Generative {Decision} {Making} {Under} {Uncertainty}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/26807},
	doi = {10.1609/aaai.v37i13.26807},
	abstract = {In the fields of natural language processing (NLP) and computer vision (CV), recent advances in generative modeling have led to powerful machine learning systems that can effectively learn from large labeled and unlabeled datasets. These systems, by and large, apply a uniform pretrain-finetune pipeline on sequential data streams and have achieved state-of-the-art-performance across many tasks and benchmarks. In this talk, we will present recent algorithms that extend this paradigm to sequential decision making, by casting it as an inverse problem that can be solved via deep generative models. These generative approaches are stable to train, provide a flexible interface for single- and multi-task inference, and generalize exceedingly well outside their training datasets. We instantiate these algorithms in the context of reinforcement learning and black-box optimization. Empirically, we demonstrate that these approaches perform exceedingly well on high-dimensional benchmarks outperforming the current state-of-the-art approaches based on forward models.},
	language = {en},
	number = {13},
	urldate = {2023-08-01},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Grover, Aditya},
	month = jun,
	year = {2023},
	note = {Number: 13},
	keywords = {New Faculty Highlights},
	pages = {15440--15440},
}

@article{le_explaining_2023,
	title = {Explaining the {Uncertainty} in {AI}-{Assisted} {Decision} {Making}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/26920},
	doi = {10.1609/aaai.v37i13.26920},
	abstract = {The aim of this project is to improve human decision-making using explainability; specifically, how to explain the (un)certainty of machine learning models. Prior research has used uncertainty measures to promote trust and decision-making. However, the direction of explaining why the AI prediction is confident (or not confident) in its prediction needs to be addressed. By explaining the model uncertainty, we can promote trust, improve understanding and improve decision-making for users.},
	language = {en},
	number = {13},
	urldate = {2023-08-01},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Le, Thao},
	month = jun,
	year = {2023},
	note = {Number: 13},
	keywords = {Human-Machine Teams},
	pages = {16119--16120},
	file = {Le_2023_Explaining the Uncertainty in AI-Assisted Decision Making.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\AAAI\\uncertainty\\Le_2023_Explaining the Uncertainty in AI-Assisted Decision Making.pdf:application/pdf},
}

@article{stoyanova_toward_2023,
	title = {Toward {Robust} {Uncertainty} {Estimation} with {Random} {Activation} {Functions}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/26768},
	doi = {10.1609/aaai.v37i12.26768},
	abstract = {Deep neural networks are in the limelight of machine learning with their excellent performance in many data-driven applications. However, they can lead to inaccurate predictions when queried in out-of-distribution data points, which can have detrimental effects especially in sensitive domains, such as healthcare and transportation, where erroneous predictions can be very costly and/or dangerous. Subsequently, quantifying the uncertainty of the output of a neural network is often leveraged to evaluate the confidence of its predictions, and ensemble models have proved to be effective in measuring the uncertainty by utilizing the variance of predictions over a pool of models. In this paper, we propose a novel approach for uncertainty quantification via ensembles, called Random Activation Functions (RAFs) Ensemble, that aims at improving the ensemble diversity toward a more robust estimation, by accommodating each neural network with a different (random) activation function. Extensive empirical study demonstrates that RAFs Ensemble outperforms state-of-the-art ensemble uncertainty quantification methods on both synthetic and real-world datasets in a series of regression tasks.},
	language = {en},
	number = {12},
	urldate = {2023-08-01},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Stoyanova, Yana and Ghandi, Soroush and Tavakol, Maryam},
	month = jun,
	year = {2023},
	note = {Number: 12},
	keywords = {General},
	pages = {15152--15160},
	file = {Stoyanova et al_2023_Toward Robust Uncertainty Estimation with Random Activation Functions.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\AAAI\\uncertainty\\Stoyanova et al_2023_Toward Robust Uncertainty Estimation with Random Activation Functions.pdf:application/pdf},
}

@misc{yan_active_2023,
	title = {Active {Implicit} {Object} {Reconstruction} using {Uncertainty}-guided {Next}-{Best}-{View} {Optimziation}},
	url = {http://arxiv.org/abs/2303.16739},
	doi = {10.48550/arXiv.2303.16739},
	abstract = {Actively planning sensor views during object reconstruction is crucial for autonomous mobile robots. An effective method should be able to strike a balance between accuracy and efficiency. In this paper, we propose a seamless integration of the emerging implicit representation with the active reconstruction task. We build an implicit occupancy field as our geometry proxy. While training, the prior object bounding box is utilized as auxiliary information to generate clean and detailed reconstructions. To evaluate view uncertainty, we employ a sampling-based approach that directly extracts entropy from the reconstructed occupancy probability field as our measure of view information gain. This eliminates the need for additional uncertainty maps or learning. Unlike previous methods that compare view uncertainty within a finite set of candidates, we aim to find the next-best-view (NBV) on a continuous manifold. Leveraging the differentiability of the implicit representation, the NBV can be optimized directly by maximizing the view uncertainty using gradient descent. It significantly enhances the method's adaptability to different scenarios. Simulation and real-world experiments demonstrate that our approach effectively improves reconstruction accuracy and efficiency of view planning in active reconstruction tasks. The proposed system will open source at https://github.com/HITSZ-NRSL/ActiveImplicitRecon.git.},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Yan, Dongyu and Liu, Jianheng and Quan, Fengyu and Chen, Haoyao and Fu, Mengmeng},
	month = jul,
	year = {2023},
	note = {arXiv:2303.16739 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {arXiv.org Snapshot:C\:\\Users\\guanxiuxian\\Zotero\\storage\\UKQKXQMG\\2303.html:text/html;Yan et al_2023_Active Implicit Object Reconstruction using Uncertainty-guided Next-Best-View.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\AAAI\\uncertainty\\Yan et al_2023_Active Implicit Object Reconstruction using Uncertainty-guided Next-Best-View.pdf:application/pdf},
}


@article{yi_scalable_2016,
	title = {A scalable active framework for region annotation in {3D} shape collections},
	volume = {35},
	issn = {0730-0301, 1557-7368},
	url = {https://dl.acm.org/doi/10.1145/2980179.2980238},
	doi = {10.1145/2980179.2980238},
	abstract = {Large repositories of 3D shapes provide valuable input for datadriven analysis and modeling tools. They are especially powerful once annotated with semantic information such as salient regions and functional parts. We propose a novel active learning method capable of enriching massive geometric datasets with accurate semantic region annotations. Given a shape collection and a userspeciﬁed region label our goal is to correctly demarcate the corresponding regions with minimal manual work. Our active framework achieves this goal by cycling between manually annotating the regions, automatically propagating these annotations across the rest of the shapes, manually verifying both human and automatic annotations, and learning from the veriﬁcation results to improve the automatic propagation algorithm. We use a uniﬁed utility function that explicitly models the time cost of human input across all steps of our method. This allows us to jointly optimize for the set of models to annotate and for the set of models to verify based on the predicted impact of these actions on the human efﬁciency. We demonstrate that incorporating veriﬁcation of all produced labelings within this uniﬁed objective improves both accuracy and efﬁciency of the active learning procedure. We automatically propagate human labels across a dynamic shape network using a conditional random ﬁeld (CRF) framework, taking advantage of global shape-to-shape similarities, local feature similarities, and point-to-point correspondences. By combining these diverse cues we achieve higher accuracy than existing alternatives. We validate our framework on existing benchmarks demonstrating it to be signiﬁcantly more efﬁcient at using human input compared to previous techniques. We further validate its efﬁciency and robustness by annotating a massive shape dataset, labeling over 93,000 shape parts, across multiple model classes, and providing a labeled part collection more than one order of magnitude larger than existing ones.},
	language = {en},
	number = {6},
	urldate = {2023-08-02},
	journal = {ACM Transactions on Graphics},
	author = {Yi, Li and Kim, Vladimir G. and Ceylan, Duygu and Shen, I-Chao and Yan, Mengyan and Su, Hao and Lu, Cewu and Huang, Qixing and Sheffer, Alla and Guibas, Leonidas},
	month = nov,
	year = {2016},
	pages = {1--12},
	file = {Yi 等 - 2016 - A scalable active framework for region annotation .pdf:C\:\\Users\\guanxiuxian\\Zotero\\storage\\RGKW5JIA\\Yi 等 - 2016 - A scalable active framework for region annotation .pdf:application/pdf},
}


@misc{maddox_simple_2019,
	title = {A {Simple} {Baseline} for {Bayesian} {Uncertainty} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1902.02476},
	doi = {10.48550/arXiv.1902.02476},
	abstract = {We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose approach for uncertainty representation and calibration in deep learning. Stochastic Weight Averaging (SWA), which computes the first moment of stochastic gradient descent (SGD) iterates with a modified learning rate schedule, has recently been shown to improve generalization in deep learning. With SWAG, we fit a Gaussian using the SWA solution as the first moment and a low rank plus diagonal covariance also derived from the SGD iterates, forming an approximate posterior distribution over neural network weights; we then sample from this Gaussian distribution to perform Bayesian model averaging. We empirically find that SWAG approximates the shape of the true posterior, in accordance with results describing the stationary distribution of SGD iterates. Moreover, we demonstrate that SWAG performs well on a wide variety of tasks, including out of sample detection, calibration, and transfer learning, in comparison to many popular alternatives including MC dropout, KFAC Laplace, SGLD, and temperature scaling.},
	urldate = {2023-08-07},
	publisher = {arXiv},
	author = {Maddox, Wesley and Garipov, Timur and Izmailov, Pavel and Vetrov, Dmitry and Wilson, Andrew Gordon},
	month = dec,
	year = {2019},
	note = {arXiv:1902.02476 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\guanxiuxian\\Zotero\\storage\\X73H5HID\\1902.html:text/html;Maddox et al_2019_A Simple Baseline for Bayesian Uncertainty in Deep Learning.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\AAAI\\uncertainty\\Maddox et al_2019_A Simple Baseline for Bayesian Uncertainty in Deep Learning.pdf:application/pdf},
}

@misc{tran_bayesian_2019,
	title = {Bayesian {Layers}: {A} {Module} for {Neural} {Network} {Uncertainty}},
	shorttitle = {Bayesian {Layers}},
	url = {http://arxiv.org/abs/1812.03973},
	doi = {10.48550/arXiv.1812.03973},
	abstract = {We describe Bayesian Layers, a module designed for fast experimentation with neural network uncertainty. It extends neural network libraries with drop-in replacements for common layers. This enables composition via a unified abstraction over deterministic and stochastic functions and allows for scalability via the underlying system. These layers capture uncertainty over weights (Bayesian neural nets), pre-activation units (dropout), activations ("stochastic output layers"), or the function itself (Gaussian processes). They can also be reversible to propagate uncertainty from input to output. We include code examples for common architectures such as Bayesian LSTMs, deep GPs, and flow-based models. As demonstration, we fit a 5-billion parameter "Bayesian Transformer" on 512 TPUv2 cores for uncertainty in machine translation and a Bayesian dynamics model for model-based planning. Finally, we show how Bayesian Layers can be used within the Edward2 probabilistic programming language for probabilistic programs with stochastic processes.},
	urldate = {2023-08-07},
	publisher = {arXiv},
	author = {Tran, Dustin and Dusenberry, Michael W. and van der Wilk, Mark and Hafner, Danijar},
	month = mar,
	year = {2019},
	note = {arXiv:1812.03973 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:C\:\\Users\\guanxiuxian\\Zotero\\storage\\YDU7Q3W4\\1812.html:text/html;Tran et al_2019_Bayesian Layers.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\AAAI\\uncertainty\\Tran et al_2019_Bayesian Layers.pdf:application/pdf},
}

@misc{charpentier_posterior_2020,
	title = {Posterior {Network}: {Uncertainty} {Estimation} without {OOD} {Samples} via {Density}-{Based} {Pseudo}-{Counts}},
	shorttitle = {Posterior {Network}},
	url = {http://arxiv.org/abs/2006.09239},
	doi = {10.48550/arXiv.2006.09239},
	abstract = {Accurate estimation of aleatoric and epistemic uncertainty is crucial to build safe and reliable systems. Traditional approaches, such as dropout and ensemble methods, estimate uncertainty by sampling probability predictions from different submodels, which leads to slow uncertainty estimation at inference time. Recent works address this drawback by directly predicting parameters of prior distributions over the probability predictions with a neural network. While this approach has demonstrated accurate uncertainty estimation, it requires defining arbitrary target parameters for in-distribution data and makes the unrealistic assumption that out-of-distribution (OOD) data is known at training time. In this work we propose the Posterior Network (PostNet), which uses Normalizing Flows to predict an individual closed-form posterior distribution over predicted probabilites for any input sample. The posterior distributions learned by PostNet accurately reflect uncertainty for in- and out-of-distribution data -- without requiring access to OOD data at training time. PostNet achieves state-of-the art results in OOD detection and in uncertainty calibration under dataset shifts.},
	urldate = {2023-08-07},
	publisher = {arXiv},
	author = {Charpentier, Bertrand and Zügner, Daniel and Günnemann, Stephan},
	month = oct,
	year = {2020},
	note = {arXiv:2006.09239 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\guanxiuxian\\Zotero\\storage\\3SZWBPRG\\2006.html:text/html;Charpentier et al_2020_Posterior Network.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\AAAI\\uncertainty\\Charpentier et al_2020_Posterior Network.pdf:application/pdf},
}

@misc{bilos_uncertainty_2020,
	title = {Uncertainty on {Asynchronous} {Time} {Event} {Prediction}},
	url = {http://arxiv.org/abs/1911.05503},
	doi = {10.48550/arXiv.1911.05503},
	abstract = {Asynchronous event sequences are the basis of many applications throughout different industries. In this work, we tackle the task of predicting the next event (given a history), and how this prediction changes with the passage of time. Since at some time points (e.g. predictions far into the future) we might not be able to predict anything with confidence, capturing uncertainty in the predictions is crucial. We present two new architectures, WGP-LN and FD-Dir, modelling the evolution of the distribution on the probability simplex with time-dependent logistic normal and Dirichlet distributions. In both cases, the combination of RNNs with either Gaussian process or function decomposition allows to express rich temporal evolution of the distribution parameters, and naturally captures uncertainty. Experiments on class prediction, time prediction and anomaly detection demonstrate the high performances of our models on various datasets compared to other approaches.},
	urldate = {2023-08-07},
	publisher = {arXiv},
	author = {Biloš, Marin and Charpentier, Bertrand and Günnemann, Stephan},
	month = jan,
	year = {2020},
	note = {arXiv:1911.05503 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\guanxiuxian\\Zotero\\storage\\IMPHJMC5\\1911.html:text/html;Biloš et al_2020_Uncertainty on Asynchronous Time Event Prediction.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\AAAI\\uncertainty\\Biloš et al_2020_Uncertainty on Asynchronous Time Event Prediction.pdf:application/pdf},
}

@article{bissiri_general_2016,
	title = {A {General} {Framework} for {Updating} {Belief} {Distributions}},
	volume = {78},
	issn = {1369-7412, 1467-9868},
	url = {http://arxiv.org/abs/1306.6430},
	doi = {10.1111/rssb.12158},
	abstract = {We propose a framework for general Bayesian inference. We argue that a valid update of a prior belief distribution to a posterior can be made for parameters which are connected to observations through a loss function rather than the traditional likelihood function, which is recovered under the special case of using self information loss.},
	language = {en},
	number = {5},
	urldate = {2023-08-07},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Bissiri, Pier Giovanni and Holmes, Chris and Walker, Stephen},
	month = nov,
	year = {2016},
	note = {arXiv:1306.6430 [math, stat]},
	keywords = {Mathematics - Statistics Theory, Statistics - Methodology},
	pages = {1103--1130},
	file = {Bissiri 等 - 2016 - A General Framework for Updating Belief Distributi.pdf:C\:\\Users\\guanxiuxian\\Zotero\\storage\\PCYLERX2\\Bissiri 等 - 2016 - A General Framework for Updating Belief Distributi.pdf:application/pdf},
}

@misc{osawa_practical_2019,
	title = {Practical {Deep} {Learning} with {Bayesian} {Principles}},
	url = {http://arxiv.org/abs/1906.02506},
	doi = {10.48550/arXiv.1906.02506},
	abstract = {Bayesian methods promise to fix many shortcomings of deep learning, but they are impractical and rarely match the performance of standard methods, let alone improve them. In this paper, we demonstrate practical training of deep networks with natural-gradient variational inference. By applying techniques such as batch normalisation, data augmentation, and distributed training, we achieve similar performance in about the same number of epochs as the Adam optimiser, even on large datasets such as ImageNet. Importantly, the benefits of Bayesian principles are preserved: predictive probabilities are well-calibrated, uncertainties on out-of-distribution data are improved, and continual-learning performance is boosted. This work enables practical deep learning while preserving benefits of Bayesian principles. A PyTorch implementation is available as a plug-and-play optimiser.},
	urldate = {2023-08-08},
	publisher = {arXiv},
	author = {Osawa, Kazuki and Swaroop, Siddharth and Jain, Anirudh and Eschenhagen, Runa and Turner, Richard E. and Yokota, Rio and Khan, Mohammad Emtiyaz},
	month = oct,
	year = {2019},
	note = {arXiv:1906.02506 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\guanxiuxian\\Zotero\\storage\\92AV7EY2\\1906.html:text/html;Osawa et al_2019_Practical Deep Learning with Bayesian Principles.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\AAAI\\uncertainty\\Osawa et al_2019_Practical Deep Learning with Bayesian Principles.pdf:application/pdf},
}

@misc{blundell_weight_2015,
	title = {Weight {Uncertainty} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1505.05424},
	doi = {10.48550/arXiv.1505.05424},
	abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
	urldate = {2023-08-08},
	publisher = {arXiv},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	month = may,
	year = {2015},
	note = {arXiv:1505.05424 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\guanxiuxian\\Zotero\\storage\\V9PXFQ5K\\1505.html:text/html;Blundell et al_2015_Weight Uncertainty in Neural Networks.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\AAAI\\uncertainty\\Blundell et al_2015_Weight Uncertainty in Neural Networks.pdf:application/pdf},
}

@INPROCEEDINGS{Biss_NBV,
  author={Bissmarck, Fredrik and Svensson, Martin and Tolt, Gustav},
  booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Efficient algorithms for Next Best View evaluation}, 
  year={2015},
  volume={},
  number={},
  pages={5876-5883},
  doi={10.1109/IROS.2015.7354212}
}

@INPROCEEDINGS{1087372,
  author={Connolly, C.},
  booktitle={Proceedings. 1985 IEEE International Conference on Robotics and Automation}, 
  title={The determination of next best views}, 
  year={1985},
  volume={2},
  number={},
  pages={432-435},
  doi={10.1109/ROBOT.1985.1087372}}


@article{chen_active_2011,
	title = {Active vision in robotic systems: A survey of recent developments},
	volume = {30},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/0278364911410755},
	doi = {10.1177/0278364911410755},
	shorttitle = {Active vision in robotic systems},
	abstract = {In this paper we provide a broad survey of developments in active vision in robotic applications over the last 15 years. With increasing demand for robotic automation, research in this area has received much attention. Among the many factors that can be attributed to a high-performance robotic system, the planned sensing or acquisition of perceptions on the operating environment is a crucial component. The aim of sensor planning is to determine the pose and settings of vision sensors for undertaking a vision-based task that usually requires obtaining multiple views of the object to be manipulated. Planning for robot vision is a complex problem for an active system due to its sensing uncertainty and environmental uncertainty. This paper describes such problems arising from many applications, e.g. object recognition and modeling, site reconstruction and inspection, surveillance, tracking and search, as well as robotic manipulation and assembly, localization and mapping, navigation and exploration. A bundle of solutions and methods have been proposed to solve these problems in the past. They are summarized in this review while enabling readers to easily refer solution methods for practical applications. Representative contributions, their evaluations, analyses, and future research trends are also addressed in an abstract level.},
	pages = {1343--1377},
	number = {11},
	journaltitle = {The International Journal of Robotics Research},
	author = {Chen, Shengyong and Li, Youfu and Kwok, Ngai Ming},
	urldate = {2023-08-16},
	date = {2011-09-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Ltd {STM}},
}


@INPROCEEDINGS{9156855,
  author={Jiang, Chiyu and Sud, Avneesh and Makadia, Ameesh and Huang, Jingwei and Nießner, Matthias and Funkhouser, Thomas},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Local Implicit Grid Representations for 3D Scenes}, 
  year={2020},
  volume={},
  number={},
  pages={6000-6009},
  doi={10.1109/CVPR42600.2020.00604}}

@misc{chabra2020deep,
      title={Deep Local Shapes: Learning Local SDF Priors for Detailed 3D Reconstruction}, 
      author={Rohan Chabra and Jan Eric Lenssen and Eddy Ilg and Tanner Schmidt and Julian Straub and Steven Lovegrove and Richard Newcombe},
      year={2020},
      eprint={2003.10983},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{genova2020local,
      title={Local Deep Implicit Functions for 3D Shape}, 
      author={Kyle Genova and Forrester Cole and Avneesh Sud and Aaron Sarna and Thomas Funkhouser},
      year={2020},
      eprint={1912.06126},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{ABDAR2021243,
title = {A review of uncertainty quantification in deep learning: Techniques, applications and challenges},
journal = {Information Fusion},
volume = {76},
pages = {243-297},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521001081},
author = {Moloud Abdar and Farhad Pourpanah and Sadiq Hussain and Dana Rezazadegan and Li Liu and Mohammad Ghavamzadeh and Paul Fieguth and Xiaochun Cao and Abbas Khosravi and U. Rajendra Acharya and Vladimir Makarenkov and Saeid Nahavandi},
keywords = {Artificial intelligence, Uncertainty quantification, Deep learning, Machine learning, Bayesian statistics, Ensemble learning},
abstract = {Uncertainty quantification (UQ) methods play a pivotal role in reducing the impact of uncertainties during both optimization and decision making processes. They have been applied to solve a variety of real-world problems in science and engineering. Bayesian approximation and ensemble learning techniques are two widely-used types of uncertainty quantification (UQ) methods. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlights fundamental research challenges and directions associated with UQ.}
}

@misc{gal2016dropout,
      title={Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}, 
      author={Yarin Gal and Zoubin Ghahramani},
      year={2016},
      eprint={1506.02142},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@misc{lakshminarayanan2017simple,
      title={Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles}, 
      author={Balaji Lakshminarayanan and Alexander Pritzel and Charles Blundell},
      year={2017},
      eprint={1612.01474},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{zenke2017continual,
      title={Continual Learning Through Synaptic Intelligence}, 
      author={Friedemann Zenke and Ben Poole and Surya Ganguli},
      year={2017},
      eprint={1703.04200},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{replica19arxiv,
  title =   {The {R}eplica Dataset: A Digital Replica of Indoor Spaces},
  author =  {Julian Straub and Thomas Whelan and Lingni Ma and Yufan Chen and Erik Wijmans and Simon Green and Jakob J. Engel and Raul Mur-Artal and Carl Ren and Shobhit Verma and Anton Clarkson and Mingfei Yan and Brian Budge and Yajie Yan and Xiaqing Pan and June Yon and Yuyang Zou and Kimberly Leon and Nigel Carter and Jesus Briales and  Tyler Gillingham and  Elias Mueggler and Luis Pesqueira and Manolis Savva and Dhruv Batra and Hauke M. Strasdat and Renzo De Nardi and Michael Goesele and Steven Lovegrove and Richard Newcombe },
  journal = {arXiv preprint arXiv:1906.05797},
  year =    {2019}
}


@inproceedings{sun_direct_2022,
	title = {Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction},
	url = {https://ieeexplore.ieee.org/document/9879963},
	doi = {10.1109/CVPR52688.2022.00538},
	shorttitle = {Direct Voxel Grid Optimization},
	abstract = {We present a super-fast convergence approach to reconstructing the per-scene radiance field from a set of images that capture the scene with known poses. This task, which is often applied to novel view synthesis, is recently revolution-ized by Neural Radiance Field ({NeRF}) for its state-of-the-art quality and fiexibility. However, {NeRF} and its variants require a lengthy training time ranging from hours to days for a single scene. In contrast, our approach achieves {NeRF}-comparable quality and converges rapidly from scratch in less than 15 minutes with a single {GPU}. We adopt a representation consisting of a density voxel grid for scene geometry and a feature voxel grid with a shallow network for complex view-dependent appearance. Modeling with explicit and discretized volume representations is not new, but we propose two simple yet non-trivial techniques that contribute to fast convergence speed and high-quality output. First, we introduce the post-activation interpolation on voxel density, which is capable of producing sharp surfaces in lower grid resolution. Second, direct voxel density optimization is prone to suboptimal geometry solutions, so we robustify the optimization process by imposing several priors. Finally, evaluation on five inward-facing benchmarks shows that our method matches, if not surpasses, {NeRF}'s quality, yet it only takes about 15 minutes to train from scratch for a new scene. Code: https://github.com/sunset1995/{DirectVoxGO}.},
	eventtitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {5449--5459},
	booktitle = {2022 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Sun, Cheng and Sun, Min and Chen, Hwann-Tzong},
	urldate = {2023-10-31},
	date = {2022-06},
	note = {{ISSN}: 2575-7075},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\guanxiux\\Zotero\\storage\\GJZGY6PD\\9879963.html:text/html;Sun et al_2022_Direct Voxel Grid Optimization.pdf:D\:\\onedrive\\OneDrive - USTC\\ZoteroPdf\\CVPR\\Sun et al_2022_Direct Voxel Grid Optimization.pdf:application/pdf},
}

@ARTICLE{1284395,
  author={Zhou Wang and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  journal={IEEE Transactions on Image Processing}, 
  title={Image quality assessment: from error visibility to structural similarity}, 
  year={2004},
  volume={13},
  number={4},
  pages={600-612},
  doi={10.1109/TIP.2003.819861}}

@INPROCEEDINGS{8578166,
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={The Unreasonable Effectiveness of Deep Features as a Perceptual Metric}, 
  year={2018},
  volume={},
  number={},
  pages={586-595},
  doi={10.1109/CVPR.2018.00068}}


@misc{ash_deep_2020,
	title = {Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds},
	url = {http://arxiv.org/abs/1906.03671},
	doi = {10.48550/arXiv.1906.03671},
	abstract = {We design a new algorithm for batch active learning with deep neural network models. Our algorithm, Batch Active learning by Diverse Gradient Embeddings ({BADGE}), samples groups of points that are disparate and high-magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, {BADGE} trades off between diversity and uncertainty without requiring any hand-tuned hyperparameters. We show that while other approaches sometimes succeed for particular batch sizes or architectures, {BADGE} consistently performs as well or better, making it a versatile option for practical active learning problems.},
	number = {{arXiv}:1906.03671},
	publisher = {{arXiv}},
	author = {Ash, Jordan T. and Zhang, Chicheng and Krishnamurthy, Akshay and Langford, John and Agarwal, Alekh},
	urldate = {2023-09-01},
	date = {2020-02-23},
	eprinttype = {arxiv},
	eprint = {1906.03671 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\guanxiux\\Zotero\\storage\\ATM9RKFZ\\1906.html:text/html;Ash et al_2020_Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\active learning\\Ash et al_2020_Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds.pdf:application/pdf},
}


@misc{jiang_fantastic_2019,
	title = {Fantastic Generalization Measures and Where to Find Them},
	url = {http://arxiv.org/abs/1912.02178},
	doi = {10.48550/arXiv.1912.02178},
	abstract = {Generalization of deep networks has been of great interest in recent years, resulting in a number of theoretically and empirically motivated complexity measures. However, most papers proposing such measures study only a small set of models, leaving open the question of whether the conclusion drawn from those experiments would remain valid in other settings. We present the first large scale study of generalization in deep networks. We investigate more then 40 complexity measures taken from both theoretical bounds and empirical studies. We train over 10,000 convolutional networks by systematically varying commonly used hyperparameters. Hoping to uncover potentially causal relationships between each measure and generalization, we analyze carefully controlled experiments and show surprising failures of some measures as well as promising measures for further research.},
	number = {{arXiv}:1912.02178},
	publisher = {{arXiv}},
	author = {Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
	urldate = {2023-11-14},
	date = {2019-12-04},
	eprinttype = {arxiv},
	eprint = {1912.02178 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\guanxiux\\Zotero\\storage\\FPTEATV2\\Jiang 等 - 2019 - Fantastic Generalization Measures and Where to Fin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\guanxiux\\Zotero\\storage\\9SNDQC5R\\1912.html:text/html},
}


@inproceedings{kim_saal_2023,
	title = {{SAAL}: Sharpness-Aware Active Learning},
	url = {https://proceedings.mlr.press/v202/kim23c.html},
	shorttitle = {{SAAL}},
	abstract = {While deep neural networks play significant roles in many research areas, they are also prone to overfitting problems under limited data instances. To overcome overfitting, this paper introduces the first active learning method to incorporate the sharpness of loss space into the acquisition function. Specifically, our proposed method, Sharpness-Aware Active Learning ({SAAL}), constructs its acquisition function by selecting unlabeled instances whose perturbed loss becomes maximum. Unlike the Sharpness-Aware learning with fully-labeled datasets, we design a pseudo-labeling mechanism to anticipate the perturbed loss w.r.t. the ground-truth label, which we provide the theoretical bound for the optimization. We conduct experiments on various benchmark datasets for vision-based tasks in image classification, object detection, and domain adaptive semantic segmentation. The experimental results confirm that {SAAL} outperforms the baselines by selecting instances that have the potentially maximal perturbation on the loss. The code is available at https://github.com/{YoonyeongKim}/{SAAL}.},
	eventtitle = {International Conference on Machine Learning},
	pages = {16424--16440},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Kim, Yoon-Yeong and Cho, Youngjae and Jang, Joonho and Na, Byeonghu and Kim, Yeongmin and Song, Kyungwoo and Kang, Wanmo and Moon, Il-Chul},
	urldate = {2023-11-12},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:C\:\\Users\\guanxiux\\Zotero\\storage\\DI6X2AYK\\Kim 等 - 2023 - SAAL Sharpness-Aware Active Learning.pdf:application/pdf},
}


@misc{keskar_large-batch_2017,
	title = {On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
	url = {http://arxiv.org/abs/1609.04836},
	doi = {10.48550/arXiv.1609.04836},
	shorttitle = {On Large-Batch Training for Deep Learning},
	abstract = {The stochastic gradient descent ({SGD}) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say \$32\$-\$512\$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
	number = {{arXiv}:1609.04836},
	publisher = {{arXiv}},
	author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
	urldate = {2023-11-14},
	date = {2017-02-09},
	eprinttype = {arxiv},
	eprint = {1609.04836 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:C\:\\Users\\guanxiux\\Zotero\\storage\\ARX8F3BP\\Keskar 等 - 2017 - On Large-Batch Training for Deep Learning General.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\guanxiux\\Zotero\\storage\\YSBIISMZ\\1609.html:text/html},
}
