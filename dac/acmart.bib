@Misc{TeXFAQ,
  title =        {{UK} List of {\TeX} Frequently Asked Questions},
  author =       {{UK \TeX{} Users Group}},
  year =         2019,
  howpublished = {\url{https://texfaq.org}}
}

@Manual{Downes04:amsart,
  title =        {The \textsf{amsart}, \textsf{amsproc}, and
                  \textsf{amsbook} document~classes},
  author =       {Michael Downes and Barbara Beeton},
  organization = {American Mathematical Society},
  year =         2004,
  month =        aug,
  note =         {\url{http://www.ctan.org/pkg/amslatex}}
}

@Manual{Fiorio15,
  title =        {{a}lgorithm2e.sty---package for algorithms},
  author =       {Cristophe Fiorio},
  year =         2015,
  month =        oct,
  note =         {\url{http://www.ctan.org/pkg/algorithm2e}}
}

@Manual{Brito09,
  title =        {The algorithms bundle},
  author =       {Rog\'erio Brito},
  year =         2009,
  month =        aug,
  note =         {\url{http://www.ctan.org/pkg/algorithms}}
}

@Manual{Heinz15,
  title =        {The Listings Package},
  author =       {Carsten Heinz and Brooks Moses and Jobst Hoffmann},
  year =         2015,
  month =        jun,
  note =         {\url{http://www.ctan.org/pkg/listings}}
}

@Manual{Fear05,
  title =        {Publication quality tables in {\LaTeX}},
  author =       {Simon Fear},
  year =         2005,
  month =        apr,
  note =         {\url{http://www.ctan.org/pkg/booktabs}}
}

@Manual{ACMIdentityStandards,
  title =        {{ACM} Visual Identity Standards},
  organization = {Association for Computing Machinery},
  year =         2007,
  note =         {\url{http://identitystandards.acm.org}}
}

@Manual{Sommerfeldt13:Subcaption,
  title =        {The subcaption package},
  author =       {Axel Sommerfeldt},
  year =         2013,
  month =        apr,
  note =         {\url{http://www.ctan.org/pkg/subcaption}}
}

@Manual{Nomencl,
  title =        {A package to create a nomenclature},
  author =       {Boris Veytsman and Bern Schandl and Lee Netherton
                  and C. V. Radhakrishnan},
  year =         2005,
  month =        sep,
  note =         {\url{http://www.ctan.org/pkg/nomencl}}
}

@Manual{Talbot16:Glossaries,
  title =        {User Manual for glossaries.sty v4.44},
  author =       {Nicola L. C. Talbot},
  year =         2019,
  month =        dec,
  note =         {\url{http://www.ctan.org/pkg/glossaries}}
}

@Manual{Carlisle04:Textcase,
  title =        {The \textsl{textcase} package},
  author =       {David Carlisle},
  month =        oct,
  year =         2004,
  note =         {\url{http://www.ctan.org/pkg/textcase}}
}

@Manual{Braams22:Babel,
  title = 	 {Babel},
  author = 	 {Johannes L. Braams and Javier Bezos},
  year = 	 2022,
  note = 	 {\url{http://www.ctan.org/pkg/babel}}}

@misc{wang2022continual,
      title={Continual Test-Time Domain Adaptation}, 
      author={Qin Wang and Olga Fink and Luc Van Gool and Dengxin Dai},
      year={2022},
      eprint={2203.13591},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{li2022bnvfusion,
      title={BNV-Fusion: Dense 3D Reconstruction using Bi-level Neural Volume Fusion}, 
      author={Kejie Li and Yansong Tang and Victor Adrian Prisacariu and Philip H. S. Torr},
      year={2022},
      eprint={2204.01139},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{camps2022learning,
      title={Learning Deep SDF Maps Online for Robot Navigation and Exploration}, 
      author={Gadiel Sznaier Camps and Robert Dyro and Marco Pavone and Mac Schwager},
      year={2022},
      eprint={2207.10782},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}
@INPROCEEDINGS{9710205,
  author={Liu, Xiaofeng and Guo, Zhenhua and Li, Site and Xing, Fangxu and You, Jane and Kuo, C.-C. Jay and El Fakhri, Georges and Woo, Jonghye},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate}, 
  year={2021},
  volume={},
  number={},
  pages={10347-10356},
  doi={10.1109/ICCV48922.2021.01020}}
@INPROCEEDINGS{9710300,
  author={Awais, Muhammad and Zhou, Fengwei and Xu, Hang and Hong, Lanqing and Luo, Ping and Bae, Sung-Ho and Li, Zhenguo},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Adversarial Robustness for Unsupervised Domain Adaptation}, 
  year={2021},
  volume={},
  number={},
  pages={8548-8557},
  doi={10.1109/ICCV48922.2021.00845}}
@misc{ash2020deep,
      title={Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds}, 
      author={Jordan T. Ash and Chicheng Zhang and Akshay Krishnamurthy and John Langford and Alekh Agarwal},
      year={2020},
      eprint={1906.03671},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{yang2016active,
  title={Active learning using uncertainty information},
  author={Yang, Yazhou and Loog, Marco},
  booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)},
  pages={2646--2651},
  year={2016},
  organization={IEEE}
}
@article{nguyen2022measure,
  title={How to measure uncertainty in uncertainty sampling for active learning},
  author={Nguyen, Vu-Linh and Shaker, Mohammad Hossein and H{\"u}llermeier, Eyke},
  journal={Machine Learning},
  volume={111},
  number={1},
  pages={89--122},
  year={2022},
  publisher={Springer}
}
@book{monarch2021human,
  title={Human-in-the-Loop Machine Learning: Active Learning and Annotation for Human-centered AI},
  author={Monarch, R. and Munro, R. and Manning, C.D.},
  isbn={9781617296741},
  url={https://books.google.com.hk/books?id=LCh0zQEACAAJ},
  year={2021},
  publisher={Manning}
}
@misc{zhang2022nerfusion,
      title={NeRFusion: Fusing Radiance Fields for Large-Scale Scene Reconstruction}, 
      author={Xiaoshuai Zhang and Sai Bi and Kalyan Sunkavalli and Hao Su and Zexiang Xu},
      year={2022},
      eprint={2203.11283},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{mu2016informationbased,
      title={Information-based Active SLAM via Topological Feature Graphs}, 
      author={Beipeng Mu and Matthew Giamou and Liam Paull and Ali-akbar Agha-mohammadi and John Leonard and Jonathan How},
      year={2016},
      eprint={1509.08155},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}


@incollection{avidan_activenerf_2022,
	address = {Cham},
	title = {{ActiveNeRF}: {Learning} {Where} to {See} with {Uncertainty} {Estimation}},
	volume = {13693},
	isbn = {978-3-031-19826-7 978-3-031-19827-4},
	shorttitle = {{ActiveNeRF}},
	url = {https://link.springer.com/10.1007/978-3-031-19827-4_14},
	language = {en},
	urldate = {2023-08-04},
	booktitle = {Computer {Vision} – {ECCV} 2022},
	publisher = {Springer Nature Switzerland},
	author = {Pan, Xuran and Lai, Zihang and Song, Shiji and Huang, Gao},
	editor = {Avidan, Shai and Brostow, Gabriel and Cissé, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
	year = {2022},
	doi = {10.1007/978-3-031-19827-4_14},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {230--246},
	file = {Pan 等 - 2022 - ActiveNeRF Learning Where to See with Uncertainty.pdf:C\:\\Users\\guanx.DORM-WIN\\Zotero\\storage\\IL2A6DYY\\Pan 等 - 2022 - ActiveNeRF Learning Where to See with Uncertainty.pdf:application/pdf},
}


@misc{ash_deep_2020,
	title = {Deep {Batch} {Active} {Learning} by {Diverse}, {Uncertain} {Gradient} {Lower} {Bounds}},
	url = {http://arxiv.org/abs/1906.03671},
	doi = {10.48550/arXiv.1906.03671},
	abstract = {We design a new algorithm for batch active learning with deep neural network models. Our algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate and high-magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between diversity and uncertainty without requiring any hand-tuned hyperparameters. We show that while other approaches sometimes succeed for particular batch sizes or architectures, BADGE consistently performs as well or better, making it a versatile option for practical active learning problems.},
	urldate = {2023-09-01},
	publisher = {arXiv},
	author = {Ash, Jordan T. and Zhang, Chicheng and Krishnamurthy, Akshay and Langford, John and Agarwal, Alekh},
	month = feb,
	year = {2020},
	note = {arXiv:1906.03671 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\guanx.DORM-WIN\\Zotero\\storage\\ATM9RKFZ\\1906.html:text/html;Ash et al_2020_Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\active learning\\Ash et al_2020_Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds.pdf:application/pdf},
}


@misc{jin_neu-nbv_2023,
	title = {{NeU}-{NBV}: {Next} {Best} {View} {Planning} {Using} {Uncertainty} {Estimation} in {Image}-{Based} {Neural} {Rendering}},
	shorttitle = {{NeU}-{NBV}},
	url = {http://arxiv.org/abs/2303.01284},
	doi = {10.48550/arXiv.2303.01284},
	abstract = {Autonomous robotic tasks require actively perceiving the environment to achieve application-specific goals. In this paper, we address the problem of positioning an RGB camera to collect the most informative images to represent an unknown scene, given a limited measurement budget. We propose a novel mapless planning framework to iteratively plan the next best camera view based on collected image measurements. A key aspect of our approach is a new technique for uncertainty estimation in image-based neural rendering, which guides measurement acquisition at the most uncertain view among view candidates, thus maximising the information value during data collection. By incrementally adding new measurements into our image collection, our approach efficiently explores an unknown scene in a mapless manner. We show that our uncertainty estimation is generalisable and valuable for view planning in unknown scenes. Our planning experiments using synthetic and real-world data verify that our uncertainty-guided approach finds informative images leading to more accurate scene representations when compared against baselines.},
	urldate = {2023-08-02},
	publisher = {arXiv},
	author = {Jin, Liren and Chen, Xieyuanli and Rückin, Julius and Popović, Marija},
	month = jul,
	year = {2023},
	note = {arXiv:2303.01284 [cs]},
	keywords = {Computer Science - Robotics},
	annote = {Comment: Accepted to IEEE/RSJ International Conference on Robotics and Intelligent Systems (IROS) 2023},
	file = {arXiv.org Snapshot:C\:\\Users\\guanx.DORM-WIN\\Zotero\\storage\\B3ACI6AP\\2303.html:text/html;Jin et al_2023_NeU-NBV.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\AAAI\\uncertainty\\Jin et al_2023_NeU-NBV.pdf:application/pdf},
}

@misc{zha_data-centric_2023,
	title = {Data-centric {Artificial} {Intelligence}: {A} {Survey}},
	shorttitle = {Data-centric {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2303.10158},
	doi = {10.48550/arXiv.2303.10158},
	urldate = {2023-04-13},
	publisher = {arXiv},
	author = {Zha, Daochen and Bhat, Zaid Pervaiz and Lai, Kwei-Herng and Yang, Fan and Jiang, Zhimeng and Zhong, Shaochen and Hu, Xia},
	month = apr,
	year = {2023},
	note = {arXiv:2303.10158 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Databases},
	annote = {Comment: 38 pages, 6 figues, 5 tables. A companion list of data-centric AI resources is available at https://github.com/daochenzha/data-centric-AI},
	file = {arXiv.org Snapshot:C\:\\Users\\guanx.DORM-WIN\\Zotero\\storage\\VVQMDMRX\\2303.html:text/html;Zha et al_2023_Data-centric Artificial Intelligence.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\SLAM\\active learning\\Zha et al_2023_Data-centric Artificial Intelligence.pdf:application/pdf},
}

@incollection{chowdhary_natural_2020,
	address = {New Delhi},
	title = {Natural {Language} {Processing}},
	isbn = {978-81-322-3972-7},
	url = {https://doi.org/10.1007/978-81-322-3972-7_19},
	abstract = {The abundant volume of natural language text in the connected world, though having a large content of knowledge, but it is becoming increasingly difficult to disseminate it by a human to discover the knowledge/wisdom in it, specifically within any given time limits. The automated NLP is aimed to do this job effectively and with accuracy, like a human does it (for a limited of amount text). This chapter presents the challenges of NLP, progress so far made in this field, NLP applications, components of NLP, and grammar of English language—the way machine requires it. In addition, covers the specific areas like probabilistic parsing, ambiguities and their resolution, information extraction, discourse analysis, NL question-answering, commonsense interfaces, commonsense thinking and reasoning, causal-diversity, and various tools for NLP. Finally, the chapter summary, and a set of relevant exercises are presented.},
	language = {en},
	urldate = {2023-10-19},
	booktitle = {Fundamentals of {Artificial} {Intelligence}},
	publisher = {Springer India},
	author = {Chowdhary, K. R.},
	editor = {Chowdhary, K.R.},
	year = {2020},
	doi = {10.1007/978-81-322-3972-7_19},
	keywords = {Ambiguity resolution, Causal-diversity, Challenges of NLP, Commonsense interfaces, Commonsense reasoning, Commonsense thinking, Discourse analysis, Natural language parsing, Natural language processing, NL ambiguities, NLP tools, Probabilistic parsing, Question-answering},
	pages = {603--649},
	file = {Chowdhary_2020_Natural Language Processing.pdf:C\:\\Users\\guanx.DORM-WIN\\Zotero\\storage\\AKGAT6II\\Chowdhary_2020_Natural Language Processing.pdf:application/pdf},
}


@inproceedings{brown_language_2020,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'20},
	title = {Language models are few-shot learners},
	isbn = {978-1-71382-954-6},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	urldate = {2023-10-19},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = dec,
	year = {2020},
	pages = {1877--1901},
	file = {Brown et al_2020_Language models are few-shot learners.pdf:C\:\\Users\\guanx.DORM-WIN\\Zotero\\storage\\8WJJ4XVF\\Brown et al_2020_Language models are few-shot learners.pdf:application/pdf},
}


@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2023-10-19},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 100 pages},
}

@article{wilson_survey_2020,
	title = {A {Survey} of {Unsupervised} {Deep} {Domain} {Adaptation}},
	volume = {11},
	issn = {2157-6904, 2157-6912},
	url = {https://dl.acm.org/doi/10.1145/3400066},
	doi = {10.1145/3400066},
	abstract = {Deep learning has produced state-of-the-art results for a variety of tasks. While such approaches for supervised learning have performed well, they assume that training and testing data are drawn from the same distribution, which may not always be the case. As a complement to this challenge, single-source unsupervised domain adaptation can handle situations where a network is trained on labeled data from a source domain and unlabeled data from a related but different target domain with the goal of performing well at test-time on the target domain. Many single-source and typically homogeneous unsupervised deep domain adaptation approaches have thus been developed, combining the powerful, hierarchical representations from deep learning with domain adaptation to reduce reliance on potentially costly target data labels. This survey will compare these approaches by examining alternative methods, the unique and common elements, results, and theoretical insights. We follow this with a look at application areas and open research directions.},
	language = {en},
	number = {5},
	urldate = {2021-10-30},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Wilson, Garrett and Cook, Diane J.},
	month = sep,
	year = {2020},
	keywords = {Domain adaptation, deep learning, generative adversarial networks},
	pages = {1--46},
	file = {Wilson and Cook - 2020 - A Survey of Unsupervised Deep Domain Adaptation.pdf:/Users/guan/OneDrive - mail.ustc.edu.cn/ZoteroPdf/2. Read/Domain Adaptation/Wilson and Cook - 2020 - A Survey of Unsupervised Deep Domain Adaptation.pdf:application/pdf;Wilson_Cook_2020_A Survey of Unsupervised Deep Domain Adaptation.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\2. Read\\Domain Adaptation\\Wilson_Cook_2020_A Survey of Unsupervised Deep Domain Adaptation.pdf:application/pdf},
}


@inproceedings{ahmed_unsupervised_2021,
	title = {Unsupervised {Multi}-{Source} {Domain} {Adaptation} {Without} {Access} to {Source} {Data}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Ahmed_Unsupervised_Multi-Source_Domain_Adaptation_Without_Access_to_Source_Data_CVPR_2021_paper.html},
	language = {en},
	urldate = {2021-10-18},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Ahmed, Sk Miraj and Raychaudhuri, Dripta S. and Paul, Sujoy and Oymak, Samet and Roy-Chowdhury, Amit K.},
	year = {2021},
	pages = {10103--10112},
	file = {Ahmed et al_2021_Unsupervised Multi-Source Domain Adaptation Without Access to Source Data.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\2. Read\\multi-camera face recognition\\multi-view learning\\Ahmed et al_2021_Unsupervised Multi-Source Domain Adaptation Without Access to Source Data.pdf:application/pdf;Ahmed et al. - 2021 - Unsupervised Multi-Source Domain Adaptation Withou.pdf:/Users/guan/OneDrive - mail.ustc.edu.cn/ZoteroPdf/2. Read/multi-camera face recognition/multi-view learning/Ahmed et al. - 2021 - Unsupervised Multi-Source Domain Adaptation Withou.pdf:application/pdf;Snapshot:C\:\\Users\\guanx.DORM-WIN\\Zotero\\storage\\IDCSJA8X\\Ahmed_Unsupervised_Multi-Source_Domain_Adaptation_Without_Access_to_Source_Data_CVPR_2021_paper.html:text/html},
}


@inproceedings{sucar_imap_2021,
	address = {Montreal, QC, Canada},
	title = {{iMAP}: {Implicit} {Mapping} and {Positioning} in {Real}-{Time}},
	isbn = {978-1-66542-812-5},
	shorttitle = {{iMAP}},
	url = {https://ieeexplore.ieee.org/document/9710431/},
	doi = {10.1109/ICCV48922.2021.00617},
	abstract = {We show for the ﬁrst time that a multilayer perceptron (MLP) can serve as the only scene representation in a realtime SLAM system for a handheld RGB-D camera. Our network is trained in live operation without prior data, building a dense, scene-speciﬁc implicit 3D model of occupancy and colour which is also immediately used for tracking.},
	language = {en},
	urldate = {2022-06-30},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Sucar, Edgar and Liu, Shikun and Ortiz, Joseph and Davison, Andrew J.},
	month = oct,
	year = {2021},
	pages = {6209--6218},
	file = {Sucar et al_2021_iMAP.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\SLAM\\explicit slam\\Sucar et al_2021_iMAP2.pdf:application/pdf},
}


@misc{zhu_nice-slam_2022,
	title = {{NICE}-{SLAM}: {Neural} {Implicit} {Scalable} {Encoding} for {SLAM}},
	shorttitle = {{NICE}-{SLAM}},
	url = {http://arxiv.org/abs/2112.12130},
	doi = {10.48550/arXiv.2112.12130},
	abstract = {Neural implicit representations have recently shown encouraging results in various domains, including promising progress in simultaneous localization and mapping (SLAM). Nevertheless, existing methods produce over-smoothed scene reconstructions and have difficulty scaling up to large scenes. These limitations are mainly due to their simple fully-connected network architecture that does not incorporate local information in the observations. In this paper, we present NICE-SLAM, a dense SLAM system that incorporates multi-level local information by introducing a hierarchical scene representation. Optimizing this representation with pre-trained geometric priors enables detailed reconstruction on large indoor scenes. Compared to recent neural implicit SLAM systems, our approach is more scalable, efficient, and robust. Experiments on five challenging datasets demonstrate competitive results of NICE-SLAM in both mapping and tracking quality. Project page: https://pengsongyou.github.io/nice-slam},
	urldate = {2022-06-21},
	publisher = {arXiv},
	author = {Zhu, Zihan and Peng, Songyou and Larsson, Viktor and Xu, Weiwei and Bao, Hujun and Cui, Zhaopeng and Oswald, Martin R. and Pollefeys, Marc},
	month = apr,
	year = {2022},
	note = {Number: arXiv:2112.12130
arXiv:2112.12130 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2022, first two authors contributed equally. Project page: https://pengsongyou.github.io/nice-slam},
	file = {arXiv.org Snapshot:C\:\\Users\\guanx.DORM-WIN\\Zotero\\storage\\UJZXD2F6\\2112.html:text/html;Zhu et al_2022_NICE-SLAM.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\SLAM\\explicit slam\\Zhu et al_2022_NICE-SLAM.pdf:application/pdf},
}



@inproceedings{wang_continual_2022,
	title = {Continual {Learning} with {Lifelong} {Vision} {Transformer}},
	doi = {10.1109/CVPR52688.2022.00027},
	abstract = {Continual learning methods aim at training a neural network from sequential data with streaming labels, relieving catastrophic forgetting. However, existing methods are based on and designed for convolutional neural networks (CNNs), which have not utilized the full potential of newly emerged powerful vision transformers. In this paper, we propose a novel attention-based framework Lifelong Vision Transformer (LVT), to achieve a better stability-plasticity trade-off for continual learning. Specifically, an inter-task attention mechanism is presented in LVT, which implicitly absorbs the previous tasks' information and slows down the drift of important attention between previous tasks and the current task. LVT designs a dual-classifier structure that independently injects new representation to avoid catas-trophic interference and accumulates the new and previous knowledge in a balanced manner to improve the overall performance. Moreover, we develop a confidence-aware memory update strategy to deepen the impression of the previous tasks. The extensive experimental results show that our approach achieves state-of-the-art performance with even fewer parameters on continual learning benchmarks.},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Wang, Zhen and Liu, Liu and Duan, Yiqun and Kong, Yajing and Tao, Dacheng},
	month = jun,
	year = {2022},
	note = {ISSN: 2575-7075},
	keywords = {Training, Machine learning, Interference, Neural networks, Benchmark testing, Computer vision, Representation learning, Learning systems, categorization, retrieval, Others, Recognition: detection, Transformers},
	pages = {171--181},
	file = {Wang et al_2022_Continual Learning with Lifelong Vision Transformer.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\SLAM\\active learning\\Wang et al_2022_Continual Learning with Lifelong Vision Transformer.pdf:application/pdf},
}


@misc{noauthor_ros_nodate,
	title = {{ROS} 2 {Documentation} — {ROS} 2 {Documentation}: {Galactic} documentation},
	url = {https://docs.ros.org/en/galactic/index.html},
	urldate = {2023-10-19},
  author =  {{ROS \TeX{} Users Group}},
  year = 2023,
}


@misc{noauthor_pytorch_nodate,
	title = {{PyTorch}},
	url = {https://pytorch.org/},
	urldate = {2023-10-19},
  author =  {{ PyTorch Foundation}},
  year = 2023,
}


@misc{noauthor_enterprise_nodate,
	title = {Enterprise {Open} {Source} and {Linux} {\textbar} {Ubuntu}},
	url = {https://ubuntu.com/},
	urldate = {2023-10-19},
	file = {Enterprise Open Source and Linux | Ubuntu:C\:\\Users\\guanx.DORM-WIN\\Zotero\\storage\\I4T6SK9J\\ubuntu.com.html:text/html},
  author =  {{ Canonical Ltd.}},
  year = 2023,
}


@techreport{straub_replica_2019,
	title = {The {Replica} {Dataset}: {A} {Digital} {Replica} of {Indoor} {Spaces}},
	shorttitle = {The {Replica} {Dataset}},
	url = {http://arxiv.org/abs/1906.05797},
	abstract = {We introduce Replica, a dataset of 18 highly photo-realistic 3D indoor scene reconstructions at room and building scale. Each scene consists of a dense mesh, high-resolution high-dynamic-range (HDR) textures, per-primitive semantic class and instance information, and planar mirror and glass reflectors. The goal of Replica is to enable machine learning (ML) research that relies on visually, geometrically, and semantically realistic generative models of the world - for instance, egocentric computer vision, semantic segmentation in 2D and 3D, geometric inference, and the development of embodied agents (virtual robots) performing navigation, instruction following, and question answering. Due to the high level of realism of the renderings from Replica, there is hope that ML systems trained on Replica may transfer directly to real world image and video data. Together with the data, we are releasing a minimal C++ SDK as a starting point for working with the Replica dataset. In addition, Replica is `Habitat-compatible', i.e. can be natively used with AI Habitat for training and testing embodied agents.},
	number = {arXiv:1906.05797},
	urldate = {2022-06-10},
	institution = {arXiv},
	author = {Straub, Julian and Whelan, Thomas and Ma, Lingni and Chen, Yufan and Wijmans, Erik and Green, Simon and Engel, Jakob J. and Mur-Artal, Raul and Ren, Carl and Verma, Shobhit and Clarkson, Anton and Yan, Mingfei and Budge, Brian and Yan, Yajie and Pan, Xiaqing and Yon, June and Zou, Yuyang and Leon, Kimberly and Carter, Nigel and Briales, Jesus and Gillingham, Tyler and Mueggler, Elias and Pesqueira, Luis and Savva, Manolis and Batra, Dhruv and Strasdat, Hauke M. and De Nardi, Renzo and Goesele, Michael and Lovegrove, Steven and Newcombe, Richard},
	month = jun,
	year = {2019},
	doi = {10.48550/arXiv.1906.05797},
	note = {arXiv:1906.05797 [cs, eess]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, Computer Science - Graphics},
	file = {arXiv.org Snapshot:C\:\\Users\\guanx.DORM-WIN\\Zotero\\storage\\XULIV49V\\1906.html:text/html;Straub et al_2019_The Replica Dataset.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\Multi-Agent SLAM\\Straub et al_2019_The Replica Dataset2.pdf:application/pdf;Straub et al_2019_The Replica Dataset.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\Multi-Agent SLAM\\Straub et al_2019_The Replica Dataset.pdf:application/pdf},
}

@inproceedings{szot2021habitat,
  title     =     {Habitat 2.0: Training Home Assistants to Rearrange their Habitat},
  author    =     {Andrew Szot and Alex Clegg and Eric Undersander and Erik Wijmans and Yili Zhao and John Turner and Noah Maestre and Mustafa Mukadam and Devendra Chaplot and Oleksandr Maksymets and Aaron Gokaslan and Vladimir Vondrus and Sameer Dharur and Franziska Meier and Wojciech Galuba and Angel Chang and Zsolt Kira and Vladlen Koltun and Jitendra Malik and Manolis Savva and Dhruv Batra},
  booktitle =     {Advances in Neural Information Processing Systems (NeurIPS)},
  year      =     {2021}
}


@article{lluvia_active_2021,
	title = {Active {Mapping} and {Robot} {Exploration}: {A} {Survey}},
	volume = {21},
	issn = {1424-8220},
	shorttitle = {Active {Mapping} and {Robot} {Exploration}},
	url = {https://www.mdpi.com/1424-8220/21/7/2445},
	doi = {10.3390/s21072445},
	abstract = {Simultaneous localization and mapping responds to the problem of building a map of the environment without any prior information and based on the data obtained from one or more sensors. In most situations, the robot is driven by a human operator, but some systems are capable of navigating autonomously while mapping, which is called native simultaneous localization and mapping. This strategy focuses on actively calculating the trajectories to explore the environment while building a map with a minimum error. In this paper, a comprehensive review of the research work developed in this ﬁeld is provided, targeting the most relevant contributions in indoor mobile robotics.},
	language = {en},
	number = {7},
	urldate = {2022-05-25},
	journal = {Sensors},
	author = {Lluvia, Iker and Lazkano, Elena and Ansuategi, Ander},
	month = apr,
	year = {2021},
	pages = {2445},
	file = {Lluvia et al_2021_Active Mapping and Robot Exploration.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\SLAM\\active slam\\Lluvia et al_2021_Active Mapping and Robot Exploration.pdf:application/pdf},
}


@article{carlone_active_2014,
	title = {Active {SLAM} and {Exploration} with {Particle} {Filters} {Using} {Kullback}-{Leibler} {Divergence}},
	volume = {75},
	issn = {1573-0409},
	url = {https://doi.org/10.1007/s10846-013-9981-9},
	doi = {10.1007/s10846-013-9981-9},
	abstract = {Autonomous exploration under uncertain robot location requires the robot to use active strategies to trade-off between the contrasting tasks of exploring the unknown scenario and satisfying given constraints on the admissible uncertainty in map estimation. The corresponding problem, namely active SLAM (Simultaneous Localization and Mapping) and exploration, has received a large attention from the robotic community for its relevance in mobile robotics applications. In this work we tackle the problem of active SLAM and exploration with Rao-Blackwellized Particle Filters. We propose an application of Kullback-Leibler divergence for the purpose of evaluating the particle-based SLAM posterior approximation. This metric is then applied in the definition of the expected information from a policy, which allows the robot to autonomously decide between exploration and place revisiting actions (i.e., loop closing). Extensive tests are performed in typical indoor and office environments and on well-known benchmarking scenarios belonging to SLAM literature, with the purpose of comparing the proposed approach with the state-of-the-art techniques and to evaluate the maturity of truly autonomous navigation systems based on particle filtering.},
	language = {en},
	number = {2},
	urldate = {2022-05-25},
	journal = {Journal of Intelligent \& Robotic Systems},
	author = {Carlone, Luca and Du, Jingjing and Kaouk Ng, Miguel and Bona, Basilio and Indri, Marina},
	month = aug,
	year = {2014},
	keywords = {Active SLAM, Autonomous exploration, Mobile robot, Rao-Blackwellized Particle Filters},
	pages = {291--311},
	file = {Carlone et al_2014_Active SLAM and Exploration with Particle Filters Using Kullback-Leibler.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\SLAM\\active slam\\Carlone et al_2014_Active SLAM and Exploration with Particle Filters Using Kullback-Leibler.pdf:application/pdf},
}


@inproceedings{mu_information-based_2016,
	title = {Information-based {Active} {SLAM} via topological feature graphs},
	doi = {10.1109/CDC.2016.7799127},
	abstract = {Exploring an unknown space and building maps is a fundamental capability for mobile robots. For fully autonomous systems, the robot would further need to actively plan its paths during exploration. The problem of designing robot trajectories to actively explore an unknown environment and minimize the map error is referred to as active simultaneous localization and mapping (active SLAM). Existing work has focused on planning paths with occupancy grid maps, which do not scale well and suffer from long term drift. This work proposes a Topological Feature Graph (TFG) representation that scales well and develops an active SLAM algorithm with it. The TFG uses graphical models, which utilize independences between variables, and enables a unified quantification of exploration and exploitation gains with a single entropy metric. Hence, it facilitates a natural and principled balance between map exploration and refinement. A probabilistic roadmap path-planner is used to generate robot paths in real time. Experimental results demonstrate that the proposed approach achieves better accuracy than a standard grid-map based approach while requiring orders of magnitude less computation and memory resources.},
	booktitle = {2016 {IEEE} 55th {Conference} on {Decision} and {Control} ({CDC})},
	author = {Mu, Beipeng and Giamou, Matthew and Paull, Liam and Agha-mohammadi, Ali-akbar and Leonard, John and How, Jonathan},
	year = {2016},
	keywords = {Trajectory, Planning, Simultaneous localization and mapping, Uncertainty, Measurement, Entropy},
	pages = {5583--5590},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\guanx.DORM-WIN\\Zotero\\storage\\U45MQITX\\7799127.html:text/html;Mu et al_2016_Information-based Active SLAM via topological feature graphs.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\SLAM\\active slam\\Mu et al_2016_Information-based Active SLAM via topological feature graphs.pdf:application/pdf},
}


@inproceedings{xu_invariant_2021,
	address = {Xi'an, China},
	title = {Invariant {EKF} based {2D} {Active} {SLAM} with {Exploration} {Task}},
	isbn = {978-1-72819-077-8},
	url = {https://ieeexplore.ieee.org/document/9561951/},
	doi = {10.1109/ICRA48506.2021.9561951},
	abstract = {Right invariant extended Kalman ﬁlter (RIEKF) based simultaneous localization and mapping (SLAM) proposed recently has shown to be able to produce more consistent SLAM estimates as compared with traditional EKF based SLAM methods, including some improved EKF SLAM methods such as observability constrained-EKF (OC-EKF) SLAM. Latest results have demonstrated that its performance is very close to optimization based SLAM algorithms such as iSAM. In this paper, we propose to use RIEKF SLAM algorithm in active SLAM where both the predicted SLAM results for choosing control actions and the actual estimated SLAM results applying the selected control actions are computed using RIEKF algorithms. The advantages over traditional EKF based active SLAM are the more accurate and consistent predicted uncertainty estimates which result in robustness of the active SLAM algorithm. The advantages over optimization based active SLAM is the reduced computational cost. Simulation results are presented to validate the advantages of the proposed algorithm3.},
	language = {en},
	urldate = {2022-08-03},
	booktitle = {2021 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Xu, Mengya and Song, Yang and Chen, Yongbo and Huang, Shoudong and Hao, Qi},
	month = may,
	year = {2021},
	pages = {5350--5356},
	file = {Xu et al_2021_Invariant EKF based 2D Active SLAM with Exploration Task.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\SLAM\\active slam\\Xu et al_2021_Invariant EKF based 2D Active SLAM with Exploration Task.pdf:application/pdf},
}


@inproceedings{guan_rog_2022,
	title = {{ROG}: {A} {High} {Performance} and {Robust} {Distributed} {Training} {System} for {Robotic} {IoT}},
	shorttitle = {{ROG}},
	doi = {10.1109/MICRO56248.2022.00032},
	abstract = {Critical robotic tasks such as rescue and disaster response are more prevalently leveraging ML (Machine Learning) models deployed on a team of wireless robots, on which data parallel (DP) training over Internet of Things of these robots (robotic IoT) can harness the distributed hardware resources to adapt their models to changing environments as soon as possible. Unfortunately, due to the need for DP synchronization across all robots, the instability in wireless networks (i.e., fluctuating bandwidth due to occlusion and varying communication distance) often leads to severe stall of robots, which affects the training accuracy within a tight time budget and wastes energy stalling. Existing methods to cope with the instability of datacenter networks are incapable of handling such straggler effect. That is because they are conducting model-granulated transmission scheduling, which is much more coarse-grained than the granularity of transient network instability in real-world robotic IoT networks, making a previously reached schedule mismatch with the varying bandwidth during transmission. We present ROG, the first ROw-Granulated distributed training system optimized for ML training over unstable wireless networks. ROG confines the granularity of transmission and synchronization to each row of a layer’s parameters and schedules the transmission of each row adaptively to the fluctuating bandwidth. In this way the ML training process can update partial and the most important gradients of a stale robot to avoid triggering stalls, while provably guaranteeing convergence. The evaluation shows that, given the same training time, ROG achieved about 4.9\% 6.5\% training accuracy gain compared with the baselines and saved 20.4\% 50.7\% of the energy to achieve the same training accuracy.},
	booktitle = {2022 55th {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture} ({MICRO})},
	author = {Guan, Xiuxian and Sun, Zekai and Deng, Shengliang and Chen, Xusheng and Zhao, Shixiong and Zhang, Zongyuan and Duan, Tianyang and Wang, Yuexuan and Wu, Chenshu and Cui, Yong and Zhang, Libo and Wu, Yanjun and Wang, Rui and Cui, Heming},
	month = oct,
	year = {2022},
	keywords = {Adaptation models, Training, Synchronization, Data models, Wireless networks, Bandwidth, distributed training, energy efficient, robust, Schedules, training throughput, wireless networks},
	pages = {336--353},
	file = {Guan et al_2022_ROG.pdf:D\:\\Onedrive\\OneDrive - USTC\\PdfCollection\\MY\\Guan et al_2022_ROG.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\guanx.DORM-WIN\\Zotero\\storage\\9N5X8S5P\\9923782.html:text/html},
}
